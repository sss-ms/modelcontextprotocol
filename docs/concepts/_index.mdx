---
title: 规范（最新版本）
cascade:
  type: docs
breadcrumbs: false
weight: 10
aliases:
  - /latest
---

{{< callout type="info" >}} **协议版本**: {{< param protocolRevision >}}
{{< /callout >}}

[Model Context Protocol](https://modelcontextprotocol.io) (MCP) 是一个开放协议，它能够实现 LLM 应用程序与外部数据源和工具之间的无缝集成。无论您是在构建 AI 驱动的 IDE、增强聊天界面，还是创建自定义 AI 工作流，MCP 都为 LLM 提供了一种标准化的方式来获取所需的上下文。

本规范基于 [schema.ts](https://github.com/modelcontextprotocol/specification/blob/main/schema/2024-11-05/schema.ts) 中的 TypeScript schema，定义了权威的协议要求。

有关实现指南和示例，请访问 [modelcontextprotocol.io](https://modelcontextprotocol.io)。

本文档中的关键词 "MUST"、"MUST NOT"、"REQUIRED"、"SHALL"、"SHALL NOT"、"SHOULD"、"SHOULD NOT"、"RECOMMENDED"、"NOT RECOMMENDED"、"MAY" 和 "OPTIONAL" 应按照 [BCP 14](https://datatracker.ietf.org/doc/html/bcp14) [[RFC2119](https://datatracker.ietf.org/doc/html/rfc2119)] [[RFC8174](https://datatracker.ietf.org/doc/html/rfc8174)] 中的描述进行解释，且仅当这些词以全大写形式出现时才具有这些特定含义。

## 概述

MCP 为应用程序提供了标准化的方式来：

- 与语言模型共享上下文信息
- 向 AI 系统暴露工具和功能
- 构建可组合的集成和工作流

协议使用 [JSON-RPC](https://www.jsonrpc.org/) 2.0 消息在以下组件之间建立通信：

- **宿主（Hosts）**：发起连接的 LLM 应用程序
- **客户端（Clients）**：宿主应用程序内的连接器
- **服务器（Servers）**：提供上下文和功能的服务

MCP 的设计部分借鉴了 [Language Server Protocol](https://microsoft.github.io/language-server-protocol/)，后者标准化了在整个开发工具生态系统中添加编程语言支持的方式。类似地，MCP 标准化了如何将额外的上下文和工具集成到 AI 应用程序生态系统中。

## 关键细节

### 基础协议

- [JSON-RPC](https://www.jsonrpc.org/) 消息格式
- 有状态连接
- 服务器和客户端功能协商

### 功能特性

服务器可以向客户端提供以下任何功能：

- **Resources**：供用户或 AI 模型使用的上下文和数据
- **Prompts**：用户的模板化消息和工作流
- **Tools**：供 AI 模型执行的函数

客户端可以向服务器提供以下功能：

- **Sampling**：服务器发起的主动行为和递归 LLM 交互

### 附加工具

- 配置
- 进度跟踪
- 取消操作
- 错误报告
- 日志记录

## 安全性和信任与安全

Model Context Protocol 通过任意数据访问和代码执行路径启用强大的功能。这种能力带来了重要的安全性和信任考虑，所有实现者都必须仔细处理这些问题。

### 关键原则

1. **用户同意和控制**

   - 用户必须明确同意并理解所有数据访问和操作
   - 用户必须保持对共享数据和执行操作的控制
   - 实现者应提供清晰的用户界面以供审查和授权活动

2. **数据隐私**

   - 宿主必须在向服务器暴露用户数据之前获得明确同意
   - 未经用户同意，宿主不得传输资源数据到其他地方
   - 用户数据应受到适当的访问控制保护

3. **工具安全**

   - 工具代表任意代码执行，必须谨慎对待
   - 宿主必须在调用任何工具之前获得明确的用户同意
   - 用户应在授权使用之前了解每个工具的功能

4. **LLM 采样控制**
   - 用户必须明确批准任何 LLM 采样请求
   - 用户应控制：
     - 是否进行采样
     - 实际发送的提示词
     - 服务器可以看到的结果
   - 协议有意限制服务器对提示词的可见性

### 实现指南

虽然 MCP 本身无法在协议层面强制执行这些安全原则，但实现者 **SHOULD**：

1. 在其应用程序中构建健壮的同意和授权流程
2. 提供清晰的安全影响文档
3. 实现适当的访问控制和数据保护
4. 在集成中遵循安全最佳实践
5. 在功能设计中考虑隐私影响

## 了解更多

探索每个协议组件的详细规范：

{{< cards >}} {{< card link="architecture" title="架构" icon="template" >}}
{{< card link="basic" title="基础协议" icon="code" >}}
{{< card link="server" title="服务器功能" icon="server" >}}
{{< card link="client" title="客户端功能" icon="user" >}}
{{< card link="contributing" title="贡献" icon="pencil" >}} {{< /cards >}}
---
title: 服务器功能
cascade:
  type: docs
weight: 3
---

{{< callout type="info" >}} **协议版本**: {{< param protocolRevision >}}
{{< /callout >}}

服务器通过 MCP 提供了向语言模型添加上下文的基本构建块。这些原语实现了客户端、服务器和语言模型之间的丰富交互：

- **Prompts**：引导语言模型交互的预定义模板或指令
- **Resources**：为模型提供额外上下文的结构化数据或内容
- **Tools**：允许模型执行操作或检索信息的可执行函数

每个原语可以在以下控制层次结构中概括：

| 原语 | 控制类型 | 描述 | 示例 |
| --- | --- | --- | --- |
| Prompts | User-controlled | 由用户选择调用的交互式模板 | 斜杠命令、菜单选项 |
| Resources | Application-controlled | 由客户端附加和管理的上下文数据 | 文件内容、git 历史 |
| Tools | Model-controlled | 暴露给 LLM 以执行操作的函数 | API POST 请求、文件写入 |

在下面详细探索这些关键原语：

{{< cards >}} {{< card link="prompts" title="提示词" icon="chat-alt-2" >}}
{{< card link="resources" title="资源" icon="document" >}}
{{< card link="tools" title="工具" icon="adjustments" >}} {{< /cards >}}
